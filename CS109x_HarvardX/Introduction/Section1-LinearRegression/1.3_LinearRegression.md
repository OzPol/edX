**Tip:** In linear regression, unlike non-parametric models like KNN, we explicitly assume a linear relationship between the predictor and response variables. The process involves estimating the parameters (beta 0 and beta 1) by minimizing the Mean Squared Error (MSE), which is the average of the squared differences between the observed and predicted values. 

The linear model is represented as:
\[
f(x) = \beta_0 + \beta_1 \times x
\]
The goal is to find the estimates \(\hat{\beta_0}\) and \(\hat{\beta_1}\) that minimize the loss function, defined as the MSE:
\[
MSE = \frac{1}{n} \sum_{i=1}^{n} (y_i - (\beta_0 + \beta_1 \times x_i))^2
\]
This minimization is done by calculating the partial derivatives of the loss function with respect to the parameters:
\[
\frac{\partial L}{\partial \beta_0} \quad \text{and} \quad \frac{\partial L}{\partial \beta_1}
\]
Setting these derivatives to zero and solving the resulting equations gives us the optimal values of \(\beta_0\) and \(\beta_1\):
\[
\hat{\beta_0}, \hat{\beta_1}
\]
These values define the regression line, which is the best-fit linear relationship for the data. While this exact solution works well for linear regression, more complex models often require numerical optimization techniques like gradient descent.
