## Error Evaluation
Level 2 headings may be created by course providers in the future.

Reminder
The equations in this course are displayed via a system that gives you a lot of options for how to view them. If you're having trouble seeing them, you can right-click on them to adjust the settings, or use your browser's built-in zoom feature to make all the text on your screen larger.

We created several different models in the example of different k-nearest neighbors, so we need a way to decide which model is best. We evaluate the error of our model by looking at how well the model is doing outside the data that is used to make the prediction.

We start with a set of data and randomly hide some of that data from our model. This is called a train-validation split. We use the visible part of the data (the training set) to estimate , and the hidden part (the validation set) to evaluate the model.

The one-neighbor model (k=1) used to make predictions using the training set is shown on the plot. Now, we look at the data we have not used to make the model, the validation data shown as purple crosses.

The difference between the true value (the red cross) and the prediction is called the residual.

### Observation Errors
For each observation (x_n, y_n) , the absolute values of the residuals,  r_i = \lvert{y_i - \hat{y_i}}\rvert quantify the error at each observation.

### Error Evaluation Continued
In order to quantify how well a model performs, we aggregate the errors across the data, and we call that the loss, error, or cost function. Cost usually refers to the total loss, while loss refers to a single training point.

A common loss function for quantitative outcomes is the Mean Squared Error (MSE):

### Mean Squared Error (MSE)
MSE = \frac{1}{n}\sum_{i = 1}^ {n}(y_i - \hat{y_i})^{2}

### Watch Out!
The MSE is by no means the only valid loss function, and it's not always the best one to use! Other choices for loss function include:

- Max Absolute Error
- Mean Absolute Error
- Mean Squared Error
- The square Root of the Mean of the Squared Errors (RMSE) is also commonly used.

RMSE =\sqrt {MSE} =  \sqrt {\displaystyle\frac{1}{n}\sum_{i = 1}^ {n}(y_i - \hat{y_i})^{2}}

### Other Kinds of Errors
Numerical error isn't the only kind you'll have to worry about. Sometimes the error is more fundamental. Sometimes we end up putting data - or the person the data represents - into the wrong category. Listen to Nabib as he talks about Type 1 and Type 2 errors.


**Tip:** When using the k-Nearest Neighbors (KNN) algorithm for real estate investment decisions, consider both numerical and categorical variables to enhance your model's accuracy.

- For **numerical metrics** like price or square footage, use **Euclidean distance** to calculate similarity between properties:
  \[
  \text{Euclidean Distance} = \sqrt{(x_1 - x_2)^2 + (y_1 - y_2)^2}
  \]

- For **categorical variables** like whether a property is in a city or near a school, use **Hamming distance** to account for differences in categories:
  \[
  \text{Hamming Distance} = \text{Number of differing categories}
  \]

- We can also consider exploring other distance metrics:

  - **Cosine Distance**: Measures the cosine of the angle between two vectors.
  \[
  \text{Cosine Distance} = 1 - \frac{\mathbf{A} \cdot \mathbf{B}}{\|\mathbf{A}\| \|\mathbf{B}\|}
  \]

  - **Manhattan Distance** (also known as L1 distance): The sum of the absolute differences of the coordinates.
  \[
  \text{Manhattan Distance} = |x_1 - x_2| + |y_1 - y_2|
  \]

Using these different distance metrics allows you to prioritize property visitations effectively, saving time and increasing efficiency in your investment decisions.


## Model Comparison

Now we have a way to measure the error of the model to do model comparison. We can do the same for all k's and compare the MSE. Now, since we have a measure of how well our model performs.

In the plot, we compare the MSE for different k-nearest neighbors on the validation data. Three neighbors seem to be the best model since it has the lowest MSE. However, we should be a bit careful since it is close to k=4. The reason that k=3 may be lower than k=4 may be due to the original decision on how we split the data between train and validation.

Which model is the best? k=3 seems to be the best model.

![Model Comparison](../Exercises/Images/S1.2-2.png)

## Model Fitness

We now have a way to compare models. But just because a model is the best does not mean that the model is good. For a subset of the data, with our best model of k=3, we calculate the MSE to be 5.0. Is that good enough? What if instead we measure sales in units of individual sales as opposed to thousand units? For k=3, the MSE is now 5,004.93. Is the MSE now good enough?

In order to answer this question, we create a scale to compare our model to a very bad model and a very good model.

We will use the simplest model, the average or naïve model for comparison:
\[
\hat{y} = \frac{1}{n}\sum_{i}^{}y_{i}
\]
is the worst possible model we can do that still makes sense.

We will say that
\[
\hat{y_i} = y_i
\]
is the best possible model, when the prediction is identical to the true value.

## R-squared

We put that into a scale from 0 to 1 by creating a new quantity R-squared.

\[
\hat{R}^{2} = 1 - \frac{\sum^{}_{i}(\hat{y_{i}}-{y}_{i})^{2}}{\sum^{}_{i}(\bar{y}-y_{i})^{2}}
\]

### Watch Out!
Though it is called R‑squared, it is not just the square of R. You can get negative R‑squared values.

- If our model is as good as the mean value, \(\bar{y}\), then R-squared=0.
- If our model is perfect, then R-squared=1.
- R-squared can be negative if the model is worse than the average. This can happen when we evaluate the model on the validation set.


**Tip:** If your R-squared value or correlation coefficient is low, it doesn't necessarily mean there is no relationship between your variables. The correlation coefficient assumes a linear relationship, so a low value may simply indicate that a linear model isn't capturing the complexity of the relationship. Before concluding that there is no connection, consider exploring other types of relationships. Plotting your data can help reveal patterns that a simple linear model might miss. Remember, a low correlation or R-squared value is just the start of your exploratory data analysis, not the final word.


In the example shared by Angela, she initially assumed that there was no relationship between her predictor and response variable because both the correlation coefficient and the beta value from a linear regression were close to zero. However, she was skeptical because she intuitively believed there should be a connection. By plotting the data, Angela discovered a pattern that a simple linear model failed to capture. This experience highlights the importance of not relying solely on numerical summaries like the correlation coefficient or R-squared value. These metrics assume a linear relationship and might miss more complex, non-linear relationships. Angela’s insight emphasizes the value of visualizing data and considering alternative models before concluding that there is no relationship between variables.
